{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z41ROzQHNISI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Carga los datos\n",
        "drive.mount('/content/drive')\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/technology_data.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/sports_data.csv')\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/entertainment_data.csv')\n",
        "df4 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/education_data.csv')\n",
        "df5 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/business_data.csv')\n",
        "\n",
        "dfs = [df1,df2,df3,df4,df5]\n",
        "df = pd.concat(dfs)\n",
        "df = shuffle(df)\n"
      ],
      "metadata": {
        "id": "vsxHpj6ONX9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea1f27b-698e-4de2-9644-9c7d6daf7c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Descargar stopwords de nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Función para mapear la columna \"category\" a valores numéricos\n",
        "def map_category(category):\n",
        "    category_map = {'business': 0, 'education': 1, 'entertainment': 2, 'sports': 3, 'technology': 4}\n",
        "    return category_map[category]\n",
        "\n",
        "# Función para realizar stemming y eliminar stopwords\n",
        "def preprocess_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Tokenización y eliminación de caracteres no alfabéticos\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    # Eliminación de stopwords y aplicación de stemming\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Cargar los archivos de datos\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/technology_data.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/sports_data.csv')\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/entertainment_data.csv')\n",
        "df4 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/education_data.csv')\n",
        "df5 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/business_data.csv')\n",
        "\n",
        "# Combinar los DataFrames\n",
        "dfs = [df1, df2, df3, df4, df5]\n",
        "df = pd.concat(dfs)\n",
        "\n",
        "# Eliminar la columna \"url\"\n",
        "df.drop(columns=[\"url\"], inplace=True)\n",
        "\n",
        "# Mapear la columna \"category\" a valores numéricos\n",
        "df['category'] = df['category'].map(map_category)\n",
        "\n",
        "# Combinar las columnas \"headlines\", \"description\" y \"content\" en una sola columna \"Content\"\n",
        "df['Content'] = df['headlines'] + ' ' + df['description'] + ' ' + df['content']\n",
        "\n",
        "# Eliminar las columnas \"headlines\", \"description\" y \"content\"\n",
        "df.drop(['headlines', 'description', 'content'], axis=1, inplace=True)\n",
        "\n",
        "# Aplicar preprocesamiento de texto (stemming y eliminación de stopwords)\n",
        "df['Content'] = df['Content'].apply(preprocess_text)\n",
        "\n",
        "# Verificar los primeros registros del DataFrame resultante\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob39c95UU48m",
        "outputId": "e6813146-1900-4c0b-df75-e75d956e356d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   category                                            Content\n",
            "0         4  unlock scienc e ink display believ must catch ...\n",
            "1         4  reddit free educ subreddit best teacher subred...\n",
            "2         4  nintendo switch thing leak suggest larger scre...\n",
            "3         4  epic fortnit new browser engin show crack appl...\n",
            "4         4  hubbl find water vapour small exoplanet landma...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Descargar stopwords de nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Función para mapear la columna \"category\" a valores numéricos\n",
        "def map_category(category):\n",
        "    category_map = {'business': 0, 'education': 1, 'entertainment': 2, 'sports': 3, 'technology': 4}\n",
        "    return category_map[category]\n",
        "\n",
        "# Función para realizar stemming y eliminar stopwords\n",
        "def preprocess_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Tokenización y eliminación de caracteres no alfabéticos\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    # Eliminación de stopwords y aplicación de stemming\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Cargar los archivos de datos\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/technology_data.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/sports_data.csv')\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/entertainment_data.csv')\n",
        "df4 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/education_data.csv')\n",
        "df5 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BIG DATA/data/NewsArticles/business_data.csv')\n",
        "# Combinar los DataFrames\n",
        "dfs = [df1, df2, df3, df4, df5]\n",
        "df = pd.concat(dfs)\n",
        "\n",
        "# Eliminar la columna \"url\"\n",
        "df.drop(columns=[\"url\"], inplace=True)\n",
        "\n",
        "# Mapear la columna \"category\" a valores numéricos\n",
        "df['category'] = df['category'].map(map_category)\n",
        "\n",
        "# Combinar las columnas \"headlines\", \"description\" y \"content\" en una sola columna \"Content\"\n",
        "df['Content'] = df['headlines'] + ' ' + df['description'] + ' ' + df['content']\n",
        "\n",
        "# Eliminar las columnas \"headlines\", \"description\" y \"content\"\n",
        "df.drop(['headlines', 'description', 'content'], axis=1, inplace=True)\n",
        "\n",
        "# Aplicar preprocesamiento de texto (stemming y eliminación de stopwords)\n",
        "df['Content'] = df['Content'].apply(preprocess_text)\n",
        "\n",
        "# Separar los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['Content']\n",
        "Y = df['category']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
        "\n",
        "# Vectorizar los datos de texto usando TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Entrenar un modelo SVM\n",
        "svm_model = SVC(kernel='linear', random_state=42)  # Selecciona el kernel adecuado según tus necesidades\n",
        "svm_model.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "Y_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "f1 = f1_score(Y_test, Y_pred, average='weighted')\n",
        "classification_rep = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsRWdOYxVl-R",
        "outputId": "56e31d45-6318-4f51-b4db-ec4b28b1fd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9845\n",
            "F1 Score: 0.9845241316912012\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       400\n",
            "           1       1.00      0.98      0.99       400\n",
            "           2       1.00      1.00      1.00       400\n",
            "           3       1.00      1.00      1.00       400\n",
            "           4       0.96      0.97      0.97       400\n",
            "\n",
            "    accuracy                           0.98      2000\n",
            "   macro avg       0.98      0.98      0.98      2000\n",
            "weighted avg       0.98      0.98      0.98      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(headlines, description, content):\n",
        "    # Combinar el titular, la descripción y el contenido en una sola cadena\n",
        "    text = headlines + ' ' + description + ' ' + content\n",
        "\n",
        "    # Preprocesamiento del texto\n",
        "    def preprocess_text(text):\n",
        "        stemmer = PorterStemmer()\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = nltk.word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha()]\n",
        "        words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "        return ' '.join(words)\n",
        "\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    # Utilizar el modelo entrenado para predecir la categoría de la noticia\n",
        "    val = svm_model.predict(tfidf_vectorizer.transform([text]))\n",
        "    val = val_to_category(int(val[0]))\n",
        "\n",
        "    # Mostrar la categoría predicha\n",
        "    print(\"Según el texto, esta noticia es sobre:\", val)\n",
        "\n",
        "# Función para convertir el valor predicho en una categoría legible\n",
        "def val_to_category(val):\n",
        "    category_map = {0: 'business', 1: 'education', 2: 'entertainment', 3: 'sports', 4: 'technology'}\n",
        "    return category_map[val]\n",
        "\n",
        "# Ejemplo de uso de la función make_predictions\n",
        "make_predictions(\"Titular de la noticia\", \"Descripción de la noticia\", \"Contenido de la noticia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBD4pGF7Yhhe",
        "outputId": "e758c556-39cf-4685-c6de-7d0404b52665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Según el texto, esta noticia es sobre: sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_headline = \"Sport advances\"\n",
        "example_description = \"New sport released\"\n",
        "example_content = \"The sport is played on Mexico.\"\n",
        "print(make_predictions(example_headline, example_description, example_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWvqpnOlYl1v",
        "outputId": "d5b99f91-1fea-48ea-c71f-6e1968329115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Según el texto, esta noticia es sobre: sports\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}